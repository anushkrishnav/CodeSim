{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask \n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import time \n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances, cosine_similarity\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def encode_code(code, model):\n",
    "    model = SentenceTransformer(model)\n",
    "    code = code.to_list()\n",
    "    return model.encode(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: sentence-transformers/all-MiniLM-L12-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_cosine_similarity(embeddings_1, embeddings_2):\n",
    "    similarity = []\n",
    "    for i in range(len(embeddings_1)):\n",
    "        sim = cosine_similarity(embeddings_1[i].reshape(1, -1), embeddings_2[i].reshape(1, -1))[0][0]\n",
    "        if sim > 0.5:\n",
    "            similarity.append(1)\n",
    "        similarity.append(0)\n",
    "    return similarity\n",
    "\n",
    "def get_soft_cosine_similarity(embeddings_1, embeddings_2):\n",
    "    similarity = []\n",
    "    for i in range(len(embeddings_1)):\n",
    "        sim = cosine_similarity(embeddings_1[i].reshape(1, -1), embeddings_2[i].reshape(1, -1))[0][0]\n",
    "        sim = softmax(sim)\n",
    "        if sim > 0.5:\n",
    "            similarity.append(1)\n",
    "        similarity.append(0)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def get_similarity(df, model):\n",
    "    code_1 = df['code1']\n",
    "    code_2 = df['code2']\n",
    "    embeddings_1 = []\n",
    "    embeddings_2 = []\n",
    "    embeddings_1 = code_1.map_partitions(encode_code, model).compute(scheduler='processes')\n",
    "    embeddings_2 = code_2.map_partitions(encode_code, model).compute(scheduler='processes')\n",
    "    # save the embeddings locally\n",
    "    np.save(f\"../data/embed/{model.replace('/', '_')}_embeddings_1.npy\", embeddings_1)\n",
    "    np.save(f\"../data/embed/{model.replace('/', '_')}_embeddings_2.npy\", embeddings_2)\n",
    "    \n",
    "    # return get_cosine_similarity(embeddings_1, embeddings_2)\n",
    "    return [embeddings_1, embeddings_2]\n",
    "\n",
    "def validate(sim, y):\n",
    "    accuracy = accuracy_score(y, sim)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    f1 = f1_score(y, sim)\n",
    "    print(f\"F1: {f1}\")\n",
    "    precision = precision_score(y, sim)\n",
    "    print(f\"Precision: {precision}\")\n",
    "    recall = recall_score(y, sim)\n",
    "    print(f\"Recall: {recall}\")\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "def peform_experiment(ddf, model):\n",
    "    temp_df = ddf.copy()\n",
    "    # scatter the data\n",
    "    temp_df = temp_df.persist()\n",
    "    sim = get_similarity(temp_df, model)\n",
    "    # return validate(sim, y)\n",
    "    return sim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ddf = dd.read_parquet(\"../data/val/clone-detection-600k-5fold.parquet\") \n",
    "ddf_100 = ddf.copy()\n",
    "models = [\n",
    "    'sentence-transformers/all-MiniLM-L12-v1',\n",
    "    \"davanstrien/code-prompt-similarity-model\",\n",
    "    'annakotarba/sentence-similarity',\n",
    "]\n",
    "# copy the keys of the models dictionary to benchmark\n",
    "benchmark_dict = {}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Model: {model}\")\n",
    "    start = time.time()\n",
    "    benchmark_dict[model] = peform_experiment(ddf_100, model)\n",
    "    print(f\"Time: {time.time()-start}\")\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\"../data/val/clone-detection-600k-5fold.parquet\") \n",
    "ddf_100 = ddf.sample(frac=0.006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def tokenize_pair(code_1, code_2):\n",
    "    return tokenizer(code_1, code_2, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "class CodePairsDataset(Dataset):\n",
    "    def __init__(self, codes1, codes2, labels):\n",
    "        self.codes1 = codes1\n",
    "        self.codes2 = codes2\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.codes1[idx], self.codes2[idx], return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        return {**{k: v.squeeze(0) for k, v in encoding.items()}, 'labels': torch.tensor(self.labels[idx])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_100 \n",
    "y_test = ddf_100['similar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     12\u001b[0m         true_labels\u001b[38;5;241m.\u001b[39mextend(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 13\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(true_labels, predictions)\n\u001b[1;32m     14\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(true_labels, predictions)\n\u001b[1;32m     15\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(true_labels, predictions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('../tuned-code-bert', num_labels=2)\n",
    "val_dataset = CodePairsDataset(ddf_100 ['code1'].values, ddf_100 ['code2'].values, y_test.values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "device = \"cpu\"\n",
    "predictions, true_labels = [], []\n",
    "for batch in val_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, axis=-1).tolist())\n",
    "        true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.93188280380764\n",
      "Validation Precision: 0.9539267015706806\n",
      "Validation Recall: 0.906693207265489\n",
      "Validation F1: 0.9297104222477357\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(f\"Validation Precision: {precision}\")\n",
    "print(f\"Validation Recall: {recall}\")\n",
    "print(f\"Validation F1: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CodeSim-1gy_qMWa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
